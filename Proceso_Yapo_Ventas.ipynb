{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Proceso Yapo - Ventas.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgobr4D4z4FZ"
      },
      "source": [
        "# <strong>Proceso:</strong> Prueba Yapo.cl\n",
        "## <strong>Objetivo:</strong>\n",
        "##### Generar un archivo Json con las series de tiempo por producto, tomando en cuenta la fecha y la cantidad de productos vendidos ese día."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_8sXzCmvxnC"
      },
      "source": [
        "## <h2>Descargas e Importaciones Iniciales:</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SDFsBDcE3Kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5165bc8-87a5-4921-9888-91f3443add32"
      },
      "source": [
        "#Establece conexión a cuenta de google drive que contiene la data input\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#Descarga de java jdk\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Descarga Apache Spark\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "#Descomprime Tar\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "#Instala Findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "#Define Variables de Entorno\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZN9rIlQO8V7"
      },
      "source": [
        "## Definición de Funciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4sSCvB1O7Oq"
      },
      "source": [
        "#----- Función de inicio de sesión Spark -----\n",
        "def init_spark():\n",
        "  spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"YapoCL\")\\\n",
        "        .getOrCreate()\n",
        "  return spark\n",
        "\n",
        "#----- Funcion que unifica todos los csv en un solo Dataframe -----\n",
        "def data_merging(main_path, folders, egCSV_name):\n",
        "  for folder in folders:\n",
        "    month_files = os.listdir(main_path+folder)\n",
        "    #Lectura del CSV que contiene los headers\n",
        "    df_eg = spark.read.options(header='True', inferSchema='True', delimiter=',')\\\n",
        "                .csv(main_path+folder+\"/\"+egCSV_name)\n",
        "    \n",
        "    mySchema = df_eg.schema\n",
        "    #Elimina CSV que ya fue leído de la lista de CSV por leer\n",
        "    month_files.remove(egCSV_name)\n",
        "\n",
        "    #Construye rutas completas (path) de los csv para su lectura\n",
        "    for position in range(0, len(month_files)):\n",
        "      month_files[position] = main_path+folder+\"/\"+month_files[position]\n",
        "\n",
        "    #Lee la lista de CSV pendientes\n",
        "    month_df = spark.read.options(delimiter=',')\\\n",
        "                    .schema(mySchema)\\\n",
        "                    .csv(month_files)\n",
        "\n",
        "    if folder == folders[0]:\n",
        "      month_df_all = month_df.unionByName(df_eg)\n",
        "    else:\n",
        "      month_df_aux = month_df.unionByName(df_eg)\n",
        "      month_df_all = month_df_all.unionByName(month_df_aux)\n",
        "  return month_df_all\n",
        "\n",
        "#----- Función que procesa el Dataframe de ventas por productos y fechas -----\n",
        "def sales_per_product_df(month_df_all):\n",
        "  month_df_all = month_df_all.withColumn(\"creation_date\", to_date(col(\"creation_date\"), \"yyyy-MM-dd\"))\\\n",
        "                           .groupBy(\"product_name\", \"creation_date\")\\\n",
        "                           .agg(count(col(\"product_name\")).alias(\"num_of_sales\"))\\\n",
        "                           .sort(\"product_name\", \"creation_date\")\n",
        "\n",
        "  return month_df_all\n",
        "\n",
        "#----- Función que genera el diccionario de ventas por producto a partir del Dataframe previamente generado -----\n",
        "def sales_per_product_dict(month_df_all, data_dict):\n",
        "  product_list = month_df_all.select(\"product_name\").distinct().collect()\n",
        "\n",
        "  for product in product_list:\n",
        "    \n",
        "    date_list = month_df_all.filter(col(\"product_name\")==product.product_name).select(\"creation_date\").collect()\n",
        "    sales_list = month_df_all.filter(col(\"product_name\")==product.product_name).select(\"num_of_sales\").collect()\n",
        "    for elem in range(0, len(date_list) ):\n",
        "      if elem == 0:\n",
        "        data_dict[product.product_name] = {}\n",
        "      data_dict[product.product_name][date_list[elem].creation_date.strftime(\"%Y-%m-%d\")] = sales_list[elem].num_of_sales\n",
        "\n",
        "  \n",
        "  return data_dict\n",
        "\n",
        "#----- Función que escribe archivo Json a partir del diccionario creado -----\n",
        "def dict_to_jsonFile(data_dict, file_name, main_path):\n",
        "  with open(main_path+file_name, \"w\") as output:\n",
        "    json.dump(data_dict, output, indent=6)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61EyxnBpPZO9"
      },
      "source": [
        "### Definición de Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1mM-NrQ4Hmj"
      },
      "source": [
        "# Ruta Drive donde se encuentran los CSV\n",
        "ruta_principal = \"drive/MyDrive/Prueba Yapo/\"\n",
        "\n",
        "# Carpetas de los meses de información disponibles\n",
        "carpetas = [\"may\", \"june\"]\n",
        "\n",
        "\n",
        "# Nombre del único archivo CSV por carpeta que contiene headers\n",
        "#(debe ser el mismo para todas las carpetas definidas)\n",
        "headers_csvNombre = \"products00.csv\"\n",
        "\n",
        "# Nombre de archivo Json de salida\n",
        "nombre_output = \"sales.json\"\n",
        "\n",
        "\n",
        "#Se crea diccionario con key \"\", el cual sera eliminado previo a exportar el archivo json\n",
        "dataDict = {\"\": {}}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGOmcOh-S3KM"
      },
      "source": [
        "# Proceso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_KWuEoXS1_D"
      },
      "source": [
        "# Inicio de Spark\n",
        "spark = init_spark()\n",
        "\n",
        "# Union de CSVs\n",
        "consolidado_df = data_merging(ruta_principal, carpetas, headers_csvNombre)\n",
        "\n",
        "\n",
        "# Calculo de ventas de productos por fecha\n",
        "ventas_df = sales_per_product_df(consolidado_df)\n",
        "\n",
        "ventas_df = ventas_df.persist()\n",
        "\n",
        "# Generación del Diccionario a partir del Dataframe procesado\n",
        "dataDict = sales_per_product_dict(ventas_df, dataDict)\n",
        "#Elimina key \"\"\n",
        "del dataDict[\"\"]\n",
        "\n",
        "\n",
        "# Escritura de Archivo Json\n",
        "dict_to_jsonFile(dataDict, nombre_output, ruta_principal)\n"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}