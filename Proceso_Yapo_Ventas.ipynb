{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proceso Yapo - Ventas.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgobr4D4z4FZ"
      },
      "source": [
        "# <strong>Proceso:</strong> Prueba Yapo\n",
        "## <strong>Objetivo:</strong>\n",
        "##### Generar un archivo Json con las series de tiempo por producto, tomando en cuenta la fecha y la cantidad de productos vendidos ese día."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_8sXzCmvxnC"
      },
      "source": [
        "## <h2>Descargas e Importaciones Iniciales:</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SDFsBDcE3Kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d08ae7-6e5e-43ee-8cfd-ae5f150ff01d"
      },
      "source": [
        "#Establece conexión a cuenta de google drive que contiene la data input\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "#Descarga de java jdk\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Descarga Apache Spark\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "#Descomprime Tar\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "#Instala Findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "from datetime import datetime\n",
        "import json\n",
        "import os\n",
        "#Definine Variables de Entorno\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kggcWhQMFW4b"
      },
      "source": [
        "### Inicio Sesión de Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmbGMOy3Qk_P"
      },
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61EyxnBpPZO9"
      },
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1mM-NrQ4Hmj"
      },
      "source": [
        "main_path = \"drive/MyDrive/Prueba Yapo/\"\n",
        "\n",
        "folders = [\"may\", \"june\"]\n",
        "\n",
        "#Se crea diccionario con key \"\", el cual sera eliminado previo a exportar el archivo json\n",
        "data_dict = {\"\": {}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb31zQUxlxpI"
      },
      "source": [
        "### Unión de todos los CSV en un solo Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg5WAIFMgYgO"
      },
      "source": [
        "for folder in folders:\n",
        "  month_files = os.listdir(main_path+folder)\n",
        "  #Lectura del CSV que contiene los headers\n",
        "  df_eg = spark.read.options(header='True', inferSchema='True', delimiter=',')\\\n",
        "               .csv(main_path+folder+\"/products00.csv\")\n",
        "  \n",
        "  mySchema = df_eg.schema\n",
        "  #Elimina CSV que ya fue leído de la lista de CSV por leer\n",
        "  month_files = month_files[1:]\n",
        "\n",
        "  #Construye rutas completas (path) de los csv para su lectura\n",
        "  for position in range(0, len(month_files)):\n",
        "    month_files[position] = main_path+folder+\"/\"+month_files[position]\n",
        "\n",
        "  #Lee la lista de CSV pendientes\n",
        "  month_df = spark.read.options(delimiter=',')\\\n",
        "                  .schema(mySchema)\\\n",
        "                  .csv(month_files)\n",
        "\n",
        "  if folder == folders[0]:\n",
        "    month_df_all = month_df.unionByName(df_eg)\n",
        "  else:\n",
        "    month_df_aux = month_df.unionByName(df_eg)\n",
        "    month_df_all = month_df_all.unionByName(month_df_aux)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWbtnPEat7Wh"
      },
      "source": [
        "### Genera Dataframe con la información a escribir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Zsb58WpP8w"
      },
      "source": [
        "month_df_all = month_df_all.withColumn(\"creation_date\", to_date(col(\"creation_date\"), \"yyyy-MM-dd\"))\\\n",
        "                           .groupBy(\"product_name\", \"creation_date\")\\\n",
        "                           .agg(count(col(\"product_name\")).alias(\"num_of_sales\"))\\\n",
        "                           .sort(\"product_name\", \"creation_date\")\n",
        "\n",
        "month_df_all = month_df_all.persist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL2VfJVluSfC"
      },
      "source": [
        "### Da formato a Dataframe para su posterior escritura en Json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m16AHR2pv2H"
      },
      "source": [
        "product_list = month_df_all.select(\"product_name\").distinct().collect()\n",
        "\n",
        "for product in product_list:\n",
        "  \n",
        "  date_list = month_df_all.filter(col(\"product_name\")==product.product_name).select(\"creation_date\").collect()\n",
        "  sales_list = month_df_all.filter(col(\"product_name\")==product.product_name).select(\"num_of_sales\").collect()\n",
        "  for elem in range(0, len(date_list) ):\n",
        "    if elem == 0:\n",
        "      data_dict[product.product_name] = {}\n",
        "    data_dict[product.product_name][date_list[elem].creation_date.strftime(\"%Y-%m-%d\")] = sales_list[elem].num_of_sales\n",
        "\n",
        "#Elimina key \"\"\n",
        "del data_dict[\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27TP1YgztifJ"
      },
      "source": [
        "## Escritura de Archivo Json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tky8K6Q7pv_d"
      },
      "source": [
        "with open(main_path+\"ventas.json\", \"w\") as output:\n",
        "  json.dump(data_dict, output, indent=6)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}